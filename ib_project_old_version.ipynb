{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJkZgCXQS49+m6W3eXf5w2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Retmmmy/finding-vulnerabilities/blob/master/ib_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqzN9aK13iph",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers datasets\n",
        "!pip install pandas numpy scikit-learn\n",
        "!pip install tree-sitter tree-sitter-c tree-sitter-cpp\n",
        "!pip install wandb\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "ASTpBk154Mr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "LUg7Tp4D5vTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"DynaOuchebara/BigVul\")\n",
        "\n",
        "\n",
        "print(\"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\", dataset)\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏:\")\n",
        "print(dataset['train'][0])\n",
        "print(\"\\n–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\", dataset['train'].column_names)"
      ],
      "metadata": {
        "id": "iR2o32Ay-F7-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"DynaOuchebara/BigVul\", split='train[:5000]')\n",
        "\n",
        "\n",
        "def prepare_data_simple(dataset, max_samples=1000):\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for item in dataset:\n",
        "        if 'func_before' in item and 'vul' in item:\n",
        "            code = item['func_before']\n",
        "            label = item['vul']\n",
        "\n",
        "            if code and isinstance(code, str):\n",
        "                texts.append(code[:400])\n",
        "                labels.append(label)\n",
        "\n",
        "                if len(texts) >= max_samples:\n",
        "                    break\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "print(\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º 500 –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
        "texts, labels = prepare_data_simple(dataset, 500)\n",
        "\n",
        "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"–£—è–∑–≤–∏–º—ã—Ö: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/codebert-base\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"SAFE\", 1: \"VULNERABLE\"},\n",
        "    label2id={\"SAFE\": 0, \"VULNERABLE\": 1},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "\n",
        "print(\"\\n–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "        return item\n",
        "\n",
        "split_idx = int(len(texts) * 0.8)\n",
        "train_dataset = SimpleDataset(\n",
        "    {k: v[:split_idx] for k, v in encodings.items()},\n",
        "    labels[:split_idx]\n",
        ")\n",
        "val_dataset = SimpleDataset(\n",
        "    {k: v[split_idx:] for k, v in encodings.items()},\n",
        "    labels[split_idx:]\n",
        ")\n",
        "\n",
        "print(f\"–û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(train_dataset)}\")\n",
        "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(val_dataset)}\")\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    report_to='none',\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 8. –û–ë–£–ß–ï–ù–ò–ï\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 9. –û–¶–ï–ù–ö–ê\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\")\n",
        "for key, value in results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_examples = [\n",
        "\n",
        "    \"\"\"void vulnerable() {\n",
        "    char buffer[10];\n",
        "    gets(buffer);  // –û–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è!\n",
        "}\"\"\",\n",
        "\n",
        "\n",
        "    \"\"\"void safe() {\n",
        "    char buffer[10];\n",
        "    fgets(buffer, sizeof(buffer), stdin);  // –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
        "}\"\"\",\n",
        "\n",
        "\\\n",
        "    \"\"\"void copy_string(char *src) {\n",
        "    char dest[20];\n",
        "    strcpy(dest, src);  // –û–ø–∞—Å–Ω–æ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã\n",
        "}\"\"\"\n",
        "]\n",
        "\n",
        "print(\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:\")\n",
        "\n",
        "for i, code in enumerate(test_examples, 1):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    vuln_prob = predictions[0][1].item() * 100\n",
        "    safe_prob = predictions[0][0].item() * 100\n",
        "\n",
        "    status = \"üî¥ –£–Ø–ó–í–ò–ú–´–ô\" if vuln_prob > 50 else \"üü¢ –ë–ï–ó–û–ü–ê–°–ù–´–ô\"\n",
        "\n",
        "    print(f\"\\n–ü—Ä–∏–º–µ—Ä {i}:\")\n",
        "    print(f\"–ö–æ–¥: {code[:80]}...\")\n",
        "    print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {status}\")\n",
        "    print(f\"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—è–∑–≤–∏–º–æ—Å—Ç–∏: {vuln_prob:.1f}%\")\n",
        "    print(f\"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {safe_prob:.1f}%\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_path = \"./trained_codebert_vuln_detector\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {save_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j4XuDiYwFEl-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def prepare_balanced_data(dataset, samples_per_class=200):\n",
        "    \"\"\"–°–æ–∑–¥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\"\"\"\n",
        "    vulnerable_texts = []\n",
        "    safe_texts = []\n",
        "\n",
        "    for item in dataset:\n",
        "        if 'func_before' in item and 'vul' in item:\n",
        "            code = item['func_before']\n",
        "            label = item['vul']\n",
        "\n",
        "            if code and isinstance(code, str):\n",
        "                code_short = code[:400]  # –û–±—Ä–µ–∑–∞–µ–º\n",
        "\n",
        "                if label == 1 and len(vulnerable_texts) < samples_per_class:\n",
        "                    vulnerable_texts.append(code_short)\n",
        "                elif label == 0 and len(safe_texts) < samples_per_class:\n",
        "                    safe_texts.append(code_short)\n",
        "\n",
        "                if len(vulnerable_texts) >= samples_per_class and len(safe_texts) >= samples_per_class:\n",
        "                    break\n",
        "\n",
        "    print(f\"–£—è–∑–≤–∏–º—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(vulnerable_texts)}\")\n",
        "    print(f\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(safe_texts)}\")\n",
        "\n",
        "\n",
        "    texts = vulnerable_texts + safe_texts\n",
        "    labels = [1] * len(vulnerable_texts) + [0] * len(safe_texts)\n",
        "\n",
        "\n",
        "    import random\n",
        "    combined = list(zip(texts, labels))\n",
        "    random.shuffle(combined)\n",
        "    texts, labels = zip(*combined)\n",
        "\n",
        "    return list(texts), list(labels)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"DynaOuchebara/BigVul\", split='train[:10000]')\n",
        "\n",
        "print(\"–°–æ–∑–¥–∞–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ 200 –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞)...\")\n",
        "texts, labels = prepare_balanced_data(dataset, 200)\n",
        "\n",
        "print(f\"\\n–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "print(f\"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(texts)}\")\n",
        "print(f\"–£—è–∑–≤–∏–º—ã—Ö (1): {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
        "print(f\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö (0): {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tsJxliwZQyDc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import random\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê –ò –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"DynaOuchebara/BigVul\", split='train[:20000]')\n",
        "\n",
        "\n",
        "def create_balanced_dataset(dataset, samples_per_class=250):\n",
        "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\"\"\"\n",
        "    vulnerable = []\n",
        "    safe = []\n",
        "\n",
        "    print(\"–°–±–æ—Ä —É—è–∑–≤–∏–º—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
        "    for item in dataset:\n",
        "        if 'func_before' in item and 'vul' in item and item['vul'] == 1:\n",
        "            code = item['func_before']\n",
        "            if code and isinstance(code, str) and len(code.strip()) > 50:\n",
        "                vulnerable.append(code[:500].strip())\n",
        "                if len(vulnerable) >= samples_per_class:\n",
        "                    break\n",
        "\n",
        "    print(\"–°–±–æ—Ä –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
        "    for item in dataset:\n",
        "        if 'func_before' in item and 'vul' in item and item['vul'] == 0:\n",
        "            code = item['func_before']\n",
        "            if code and isinstance(code, str) and len(code.strip()) > 50:\n",
        "                safe.append(code[:500].strip())\n",
        "                if len(safe) >= samples_per_class:\n",
        "                    break\n",
        "\n",
        "    print(f\"\\n–°–æ–±—Ä–∞–Ω–æ:\")\n",
        "    print(f\"  –£—è–∑–≤–∏–º—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(vulnerable)}\")\n",
        "    print(f\"  –ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(safe)}\")\n",
        "\n",
        "\n",
        "    all_texts = vulnerable + safe\n",
        "    all_labels = [1] * len(vulnerable) + [0] * len(safe)\n",
        "\n",
        "\n",
        "    combined = list(zip(all_texts, all_labels))\n",
        "    random.shuffle(combined)\n",
        "    texts, labels = zip(*combined)\n",
        "\n",
        "    return list(texts), list(labels)\n",
        "\n",
        "\n",
        "texts, labels = create_balanced_dataset(dataset, 250)\n",
        "\n",
        "print(f\"\\n–ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç:\")\n",
        "print(f\"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(texts)}\")\n",
        "print(f\"–£—è–∑–≤–∏–º—ã—Ö: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
        "print(f\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö: {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "\n",
        "split_idx = int(len(texts) * 0.8)\n",
        "train_texts, val_texts = texts[:split_idx], texts[split_idx:]\n",
        "train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "train_encodings = {\n",
        "    'input_ids': encodings['input_ids'][:split_idx],\n",
        "    'attention_mask': encodings['attention_mask'][:split_idx]\n",
        "}\n",
        "val_encodings = {\n",
        "    'input_ids': encodings['input_ids'][split_idx:],\n",
        "    'attention_mask': encodings['attention_mask'][split_idx:]\n",
        "}\n",
        "\n",
        "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/codebert-base\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"SAFE\", 1: \"VULNERABLE\"},\n",
        "    label2id={\"SAFE\": 0, \"VULNERABLE\": 1}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class CodeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "        return item\n",
        "\n",
        "train_dataset = CodeDataset(train_encodings, train_labels)\n",
        "val_dataset = CodeDataset(val_encodings, val_labels)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ù–ê–°–¢–†–û–ô–ö–ê –ú–ï–¢–†–ò–ö\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "\n",
        "    f1_vulnerable = f1[1] if len(f1) > 1 else 0\n",
        "\n",
        "\n",
        "    report = classification_report(labels, predictions,\n",
        "                                   target_names=['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π', '–£—è–∑–≤–∏–º—ã–π'],\n",
        "                                   output_dict=True, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_vulnerable': f1_vulnerable,\n",
        "        'precision_safe': precision[0],\n",
        "        'recall_safe': recall[0],\n",
        "        'f1_safe': f1[0],\n",
        "        'precision_vulnerable': precision[1],\n",
        "        'recall_vulnerable': recall[1],\n",
        "        'vulnerable_support': int(labels.sum()),\n",
        "        'predicted_vulnerable': int(predictions.sum())\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./balanced_training',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_vulnerable\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    report_to='none',\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç 15-20 –º–∏–Ω—É—Ç...\")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò:\")\n",
        "for key, value in eval_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_examples = [\n",
        "\n",
        "    (\"void vulnerable_gets() { char buf[10]; gets(buf); }\", \"üî¥ gets() - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å\"),\n",
        "    (\"void vulnerable_strcpy() { char dest[10]; strcpy(dest, src); }\", \"üî¥ strcpy() –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏\"),\n",
        "    (\"int vulnerable_scanf() { char buf[10]; scanf(\\\"%s\\\", buf); return 0; }\", \"üî¥ scanf(%s) –æ–ø–∞—Å–µ–Ω\"),\n",
        "\n",
        "\n",
        "    (\"void safe_fgets() { char buf[10]; fgets(buf, 10, stdin); }\", \"üü¢ fgets() —Å –ª–∏–º–∏—Ç–æ–º\"),\n",
        "    (\"void safe_strncpy() { char dest[10]; strncpy(dest, src, 9); dest[9] = 0; }\", \"üü¢ strncpy() —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º\"),\n",
        "    (\"int safe_scanf() { char buf[10]; scanf(\\\"%9s\\\", buf); return 0; }\", \"üü¢ scanf() —Å —à–∏—Ä–∏–Ω–æ–π\"),\n",
        "]\n",
        "\n",
        "print(\"\\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –†–ê–ó–ù–´–• –ü–†–ò–ú–ï–†–ê–•:\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, (code, description) in enumerate(test_examples, 1):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    vuln_prob = predictions[0][1].item() * 100\n",
        "    safe_prob = predictions[0][0].item() * 100\n",
        "\n",
        "\n",
        "    if vuln_prob > 70:\n",
        "        status = \"üî¥ –í–´–°–û–ö–ò–ô –†–ò–°–ö –£–Ø–ó–í–ò–ú–û–°–¢–ò\"\n",
        "        color = \"red\"\n",
        "    elif vuln_prob > 40:\n",
        "        status = \"üü° –°–†–ï–î–ù–ò–ô –†–ò–°–ö\"\n",
        "        color = \"orange\"\n",
        "    else:\n",
        "        status = \"üü¢ –ù–ò–ó–ö–ò–ô –†–ò–°–ö / –ë–ï–ó–û–ü–ê–°–ù–û\"\n",
        "        color = \"green\"\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä {i}: {description}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"–ö–æ–¥: {code}\")\n",
        "    print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\")\n",
        "    print(f\"  –°—Ç–∞—Ç—É—Å: {status}\")\n",
        "    print(f\"  –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—è–∑–≤–∏–º–æ—Å—Ç–∏: {vuln_prob:.1f}%\")\n",
        "    print(f\"  –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {safe_prob:.1f}%\")\n",
        "\n",
        "\n",
        "    patterns = {\n",
        "        'gets(': 'üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û: gets() –∑–∞–ø—Ä–µ—â–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ fgets()',\n",
        "        'strcpy(': '‚ö†Ô∏è –û–ü–ê–°–ù–û: strcpy() –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ strncpy()',\n",
        "        'scanf(\"%s': '‚ö†Ô∏è –†–ò–°–ö: scanf() —Å %s. –û–≥—Ä–∞–Ω–∏—á—å—Ç–µ —à–∏—Ä–∏–Ω—É (%10s)',\n",
        "        'strcat(': '‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: strcat() –º–æ–∂–µ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—å –±—É—Ñ–µ—Ä',\n",
        "        'system(': '‚ö†Ô∏è –û–ü–ê–°–ù–û: system() –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã',\n",
        "    }\n",
        "\n",
        "    detected = []\n",
        "    for pattern, warning in patterns.items():\n",
        "        if pattern in code:\n",
        "            detected.append(warning)\n",
        "\n",
        "    if detected:\n",
        "        print(f\"\\nüîé –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã:\")\n",
        "        for warn in detected:\n",
        "            print(f\"  ‚Ä¢ {warn}\")\n",
        "\n",
        "\n",
        "\n",
        "save_path = \"./code_vulnerability_detector_final\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IEEzzOxOQ6OW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_PATH = \"./code_vulnerability_detector_final\"\n",
        "\n",
        "class SmartVulnerabilityDetector:\n",
        "    def __init__(self):\n",
        "        print(\" –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π...\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "            self.model.eval()\n",
        "            print(\"‚úÖ ML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "\n",
        "\n",
        "            print(f\"  –ö–ª–∞—Å—Å—ã: {self.model.config.id2label}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ML –º–æ–¥–µ–ª—å: {e}\")\n",
        "            print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è rule-based –¥–µ—Ç–µ–∫—Ç–æ—Ä\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def analyze_with_ml(self, code):\n",
        "        \"\"\"–ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é ML –º–æ–¥–µ–ª–∏\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                code[:500],\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "\n",
        "            vuln_prob = probs[0][1].item() * 100\n",
        "\n",
        "            return {\n",
        "                \"ml_vulnerability_score\": vuln_prob,\n",
        "                \"ml_safety_score\": 100 - vuln_prob,\n",
        "                \"raw_probs\": probs[0].tolist()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ ML –∞–Ω–∞–ª–∏–∑–∞: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_with_rules(self, code):\n",
        "        \"\"\"Rule-based –∞–Ω–∞–ª–∏–∑ (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞!)\"\"\"\n",
        "        score = 0\n",
        "        warnings = []\n",
        "        patterns_found = []\n",
        "\n",
        "\n",
        "        critical_patterns = {\n",
        "            r'gets\\s*\\(': {\n",
        "                'weight': 70,\n",
        "                'message': 'üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –£–Ø–ó–í–ò–ú–û–°–¢–¨: —Ñ—É–Ω–∫—Ü–∏—è gets() –ó–ê–ü–†–ï–©–ï–ù–ê!'\n",
        "            },\n",
        "            r'system\\s*\\([^)]*\\)': {\n",
        "                'weight': 60,\n",
        "                'message': 'üö® –í–´–°–û–ö–ò–ô –†–ò–°–ö: system() –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã'\n",
        "            },\n",
        "            r'strcpy\\s*\\(': {\n",
        "                'weight': 55,\n",
        "                'message': '‚ö†Ô∏è –í–´–°–û–ö–ò–ô –†–ò–°–ö: strcpy() –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã'\n",
        "            },\n",
        "            r'scanf\\s*\\([^)]*%s': {\n",
        "                'weight': 50,\n",
        "                'message': '‚ö†Ô∏è –í–´–°–û–ö–ò–ô –†–ò–°–ö: scanf() —Å %s –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è'\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "        medium_patterns = {\n",
        "            r'strcat\\s*\\(': {\n",
        "                'weight': 40,\n",
        "                'message': '‚ö†Ô∏è –°–†–ï–î–ù–ò–ô –†–ò–°–ö: strcat() –º–æ–∂–µ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—å –±—É—Ñ–µ—Ä'\n",
        "            },\n",
        "            r'sprintf\\s*\\([^)]*%[^)]*\\)': {\n",
        "                'weight': 35,\n",
        "                'message': '‚ö†Ô∏è –†–ò–°–ö: sprintf() –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—é'\n",
        "            },\n",
        "            r'malloc\\s*\\([^)]*\\)[^{}]*$': {\n",
        "                'weight': 30,\n",
        "                'message': '‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: malloc() –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ free()'\n",
        "            },\n",
        "            r'printf\\s*\\([^)]*%[^f]': {\n",
        "                'weight': 25,\n",
        "                'message': '‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: printf() —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º'\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "        for pattern, info in critical_patterns.items():\n",
        "            if re.search(pattern, code, re.IGNORECASE):\n",
        "                score += info['weight']\n",
        "                warnings.append(info['message'])\n",
        "                patterns_found.append(pattern)\n",
        "\n",
        "\n",
        "        for pattern, info in medium_patterns.items():\n",
        "            if re.search(pattern, code, re.IGNORECASE):\n",
        "                score += info['weight']\n",
        "                warnings.append(info['message'])\n",
        "                patterns_found.append(pattern)\n",
        "\n",
        "\n",
        "        safe_patterns = {\n",
        "            r'fgets\\s*\\(': {\n",
        "                'weight': -20,\n",
        "                'message': '‚úÖ –ë–ï–ó–û–ü–ê–°–ù–û: fgets() —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞'\n",
        "            },\n",
        "            r'strncpy\\s*\\(': {\n",
        "                'weight': -15,\n",
        "                'message': '‚úÖ –ë–ï–ó–û–ü–ê–°–ù–û: strncpy() —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –¥–ª–∏–Ω—ã'\n",
        "            },\n",
        "            r'free\\s*\\(': {\n",
        "                'weight': -10,\n",
        "                'message': '‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏'\n",
        "            },\n",
        "        }\n",
        "\n",
        "        for pattern, info in safe_patterns.items():\n",
        "            if re.search(pattern, code, re.IGNORECASE):\n",
        "                score = max(0, score + info['weight'])\n",
        "                warnings.append(info['message'])\n",
        "\n",
        "\n",
        "        probability = min(100, score)\n",
        "\n",
        "\n",
        "        if probability >= 60:\n",
        "            risk_level = \"üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô\"\n",
        "            color = \"#ff4444\"\n",
        "        elif probability >= 40:\n",
        "            risk_level = \"üü° –í–´–°–û–ö–ò–ô\"\n",
        "            color = \"#ffaa00\"\n",
        "        elif probability >= 20:\n",
        "            risk_level = \"üü† –°–†–ï–î–ù–ò–ô\"\n",
        "            color = \"#ff8800\"\n",
        "        else:\n",
        "            risk_level = \"üü¢ –ù–ò–ó–ö–ò–ô\"\n",
        "            color = \"#00cc66\"\n",
        "\n",
        "        return {\n",
        "            \"rule_vulnerability_score\": probability,\n",
        "            \"risk_level\": risk_level,\n",
        "            \"color\": color,\n",
        "            \"warnings\": warnings if warnings else [\"‚úÖ –û–ø–∞—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\"],\n",
        "            \"patterns_found\": patterns_found if patterns_found else [\"–ù–µ—Ç –æ–ø–∞—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\"]\n",
        "        }\n",
        "\n",
        "    def get_recommendations(self, code):\n",
        "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é\"\"\"\n",
        "        recommendations = []\n",
        "        code_lower = code.lower()\n",
        "\n",
        "\n",
        "        if 'gets(' in code_lower:\n",
        "            recommendations.append(\"\"\"\n",
        "            **üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –£–Ø–ó–í–ò–ú–û–°–¢–¨: gets()**\n",
        "            ```c\n",
        "            // –í–ú–ï–°–¢–û –≠–¢–û–ì–û:\n",
        "            char buffer[10];\n",
        "            gets(buffer);\n",
        "\n",
        "            // –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –≠–¢–û:\n",
        "            char buffer[10];\n",
        "            fgets(buffer, sizeof(buffer), stdin);\n",
        "            // –£–¥–∞–ª–∏—Ç–µ —Å–∏–º–≤–æ–ª –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "            buffer[strcspn(buffer, \"\\\\n\")] = 0;\n",
        "            ```\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "        if 'strcpy(' in code_lower and 'strncpy' not in code_lower:\n",
        "            recommendations.append(\"\"\"\n",
        "            **‚ö†Ô∏è –û–ü–ê–°–ù–û: strcpy() –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã**\n",
        "            ```c\n",
        "            // –í–ú–ï–°–¢–û –≠–¢–û–ì–û:\n",
        "            char dest[10];\n",
        "            strcpy(dest, src);\n",
        "\n",
        "            // –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –≠–¢–û:\n",
        "            char dest[10];\n",
        "            strncpy(dest, src, sizeof(dest) - 1);\n",
        "            dest[sizeof(dest) - 1] = '\\\\0';\n",
        "            ```\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "        if 'scanf(' in code_lower and '%s' in code:\n",
        "            recommendations.append(\"\"\"\n",
        "            **‚ö†Ô∏è –†–ò–°–ö: scanf() —Å –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –≤–≤–æ–¥–æ–º**\n",
        "            ```c\n",
        "            // –í–ú–ï–°–¢–û –≠–¢–û–ì–û:\n",
        "            char buffer[10];\n",
        "            scanf(\"%s\", buffer);\n",
        "\n",
        "            // –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –≠–¢–û:\n",
        "            char buffer[10];\n",
        "            scanf(\"%9s\", buffer);  // –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã\n",
        "\n",
        "            // –ò–õ–ò –õ–£–ß–®–ï:\n",
        "            fgets(buffer, sizeof(buffer), stdin);\n",
        "            ```\n",
        "            \"\"\")\n",
        "\n",
        "        if 'malloc(' in code_lower and 'free(' not in code_lower:\n",
        "            recommendations.append(\"\"\"\n",
        "            **‚ö†Ô∏è –£–¢–ï–ß–ö–ê –ü–ê–ú–Ø–¢–ò: malloc() –±–µ–∑ free()**\n",
        "            ```c\n",
        "            // –í–ú–ï–°–¢–û –≠–¢–û–ì–û:\n",
        "            int *arr = malloc(n * sizeof(int));\n",
        "            // ... –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ...\n",
        "\n",
        "            // –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –≠–¢–û:\n",
        "            int *arr = malloc(n * sizeof(int));\n",
        "            if (arr == NULL) {\n",
        "                // –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏\n",
        "            }\n",
        "            // ... –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ...\n",
        "            free(arr);  // –ù–µ –∑–∞–±—É–¥—å—Ç–µ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å!\n",
        "            ```\n",
        "            \"\"\")\n",
        "\n",
        "        if not recommendations:\n",
        "            recommendations.append(\"\"\"\n",
        "            **‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:**\n",
        "            - –ö–æ–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç best practices –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n",
        "            - –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
        "            - –†–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞\n",
        "            \"\"\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def analyze(self, code, threshold=30):\n",
        "        \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥)\"\"\"\n",
        "        if not code or len(code.strip()) < 10:\n",
        "            return {\"error\": \"‚ùå –í–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –¥–ª–∏–Ω–Ω–µ–µ 10 —Å–∏–º–≤–æ–ª–æ–≤\"}\n",
        "\n",
        "        print(f\"\\nüìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ ({len(code)} —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "        rule_result = self.analyze_with_rules(code)\n",
        "\n",
        "\n",
        "        ml_result = None\n",
        "        if self.model:\n",
        "            ml_result = self.analyze_with_ml(code)\n",
        "\n",
        "\n",
        "        if ml_result and ml_result[\"ml_vulnerability_score\"] > 0:\n",
        "\n",
        "            final_score = (\n",
        "                ml_result[\"ml_vulnerability_score\"] * 0.6 +\n",
        "                rule_result[\"rule_vulnerability_score\"] * 0.4\n",
        "            )\n",
        "            method = \" ML +  Rules Hybrid\"\n",
        "            ml_details = {\n",
        "                \"ml_score\": f\"{ml_result['ml_vulnerability_score']:.1f}%\",\n",
        "                \"safety_score\": f\"{ml_result['ml_safety_score']:.1f}%\"\n",
        "            }\n",
        "        else:\n",
        "\n",
        "            final_score = rule_result[\"rule_vulnerability_score\"]\n",
        "            method = \" Rule-Based Analysis\"\n",
        "            ml_details = {}\n",
        "\n",
        "\n",
        "        if final_score >= threshold:\n",
        "            final_status = \"üî¥ –£–Ø–ó–í–ò–ú–´–ô\"\n",
        "            final_color = \"#ff4444\"\n",
        "        else:\n",
        "            final_status = \"üü¢ –ë–ï–ó–û–ü–ê–°–ù–´–ô\"\n",
        "            final_color = \"#00cc66\"\n",
        "\n",
        "\n",
        "        recommendations = self.get_recommendations(code)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            \"status\": final_status,\n",
        "            \"status_color\": final_color,\n",
        "            \"final_vulnerability_score\": f\"{final_score:.1f}%\",\n",
        "            \"risk_level\": rule_result[\"risk_level\"],\n",
        "            \"analysis_method\": method,\n",
        "            \"code_info\": {\n",
        "                \"lines\": code.count('\\n') + 1,\n",
        "                \"size\": f\"{len(code)} —Å–∏–º–≤–æ–ª–æ–≤\",\n",
        "                \"language\": \"C/C++\"\n",
        "            },\n",
        "            \"warnings\": rule_result[\"warnings\"],\n",
        "            \"patterns_found\": rule_result[\"patterns_found\"],\n",
        "            \"recommendations\": recommendations,\n",
        "            \"threshold_used\": f\"{threshold}%\"\n",
        "        }\n",
        "\n",
        "\n",
        "        if ml_details:\n",
        "            result.update(ml_details)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "detector = SmartVulnerabilityDetector()\n",
        "\n",
        "\n",
        "EXAMPLES = [\n",
        "    [\"void dangerous() { char buffer[10]; gets(buffer); }\"],\n",
        "    [\"void safe_input() { char name[50]; fgets(name, 50, stdin); }\"],\n",
        "    [\"void bad_copy() { char dest[10]; strcpy(dest, src); }\"],\n",
        "    [\"void good_copy() { char dest[10]; strncpy(dest, src, 9); dest[9] = 0; }\"],\n",
        "    [\"int risky() { char cmd[100]; system(\\\"ls -la\\\"); return 0; }\"],\n",
        "]\n",
        "\n",
        "def create_interface():\n",
        "    with gr.Blocks(\n",
        "        title=\"–î–µ—Ç–µ–∫—Ç–æ—Ä —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –∫–æ–¥–µ\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .critical { color: #ff4444 !important; font-weight: bold; }\n",
        "        .warning { color: #ffaa00 !important; font-weight: bold; }\n",
        "        .safe { color: #00cc66 !important; font-weight: bold; }\n",
        "        .code-example { font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px; }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # –î–µ—Ç–µ–∫—Ç–æ—Ä —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –∫–æ–¥–µ\n",
        "\n",
        "\n",
        "        ---\n",
        "        \"\"\")\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                code_input = gr.Textbox(\n",
        "                    label=\"–í–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –Ω–∞ C/C++ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\",\n",
        "\n",
        "                    lines=15,\n",
        "                    elem_id=\"code_input\"\n",
        "                )\n",
        "\n",
        "                threshold_slider = gr.Slider(\n",
        "                    minimum=10,\n",
        "                    maximum=50,\n",
        "                    value=30,\n",
        "                    step=5,\n",
        "                    label=\" –ü–æ—Ä–æ–≥ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\",\n",
        "                    info=\"–ü—Ä–∏ –ø–æ—Ä–æ–≥–µ 30%: –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—è–∑–≤–∏–º–æ—Å—Ç–∏ > 30%, –∫–æ–¥ —Å—á–∏—Ç–∞–µ—Ç—Å—è —É—è–∑–≤–∏–º—ã–º\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    analyze_btn = gr.Button(\n",
        "                        \"–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\",\n",
        "                        elem_id=\"analyze_btn\"\n",
        "                    )\n",
        "                    clear_btn = gr.Button(\"–û—á–∏—Å—Ç–∏—Ç—å\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                output_json = gr.JSON(\n",
        "                    label=\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\",\n",
        "                    elem_id=\"results\"\n",
        "                )\n",
        "\n",
        "\n",
        "        gr.Markdown(\"### üß™ –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "        gr.Examples(\n",
        "            examples=EXAMPLES,\n",
        "            inputs=[code_input],\n",
        "            label=\"–ù–∞–∂–º–∏—Ç–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏\",\n",
        "            cache_examples=False\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        def analyze_handler(code, threshold):\n",
        "            return detector.analyze(code, threshold)\n",
        "\n",
        "        def clear_handler():\n",
        "            return \"\"\n",
        "\n",
        "        analyze_btn.click(\n",
        "            fn=analyze_handler,\n",
        "            inputs=[code_input, threshold_slider],\n",
        "            outputs=[output_json]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_handler,\n",
        "            inputs=[],\n",
        "            outputs=[code_input]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"–ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π...\")\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=False,\n",
        "        debug=True\n",
        "    )"
      ],
      "metadata": {
        "id": "RLTMv1wOuWVe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
